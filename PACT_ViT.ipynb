{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65b5a174",
   "metadata": {},
   "source": [
    "# ViT CIFAR-10\n",
    "load timm ViT and retrain on CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95ac49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import network.resnet_orig as resnet\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, RandomSampler, SequentialSampler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "model_names = sorted(name for name in resnet.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "                     and name.startswith(\"resnet\")\n",
    "                     and callable(resnet.__dict__[name]))\n",
    "\n",
    "DATA_DIR = \"train\"\n",
    "print_freq = 50\n",
    "\n",
    "# vit imsize\n",
    "imsize = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91238a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>label_enc</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>frog</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>truck</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>deer</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>automobile</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       label  label_enc  fold\n",
       "0   1        frog          6     1\n",
       "1   2       truck          9     1\n",
       "2   3       truck          9     2\n",
       "3   4        deer          4     1\n",
       "4   5  automobile          1     2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare labels\n",
    "train_df = pd.read_csv(\"trainLabels.csv\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train_df['label_enc'] = le.fit_transform(train_df['label'])\n",
    "\n",
    "# 5-fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "train_df[\"fold\"] = -1\n",
    "for i, (train_index, test_index) in enumerate(skf.split(train_df.id, train_df.label_enc)):\n",
    "    train_df.loc[test_index, 'fold'] = i\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44be87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cifarDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 rand=False,\n",
    "                 transform=None,\n",
    "                 test=False\n",
    "                ):\n",
    "\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.rand = rand\n",
    "        self.transform = transform\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        img_id = row.id\n",
    "        \n",
    "        images = cv2.imread(os.path.join(DATA_DIR, str(img_id)+\".png\"))\n",
    "        \n",
    "        # Load labels\n",
    "        label = row.label_enc\n",
    "        \n",
    "        # aug\n",
    "        if self.transform is not None:\n",
    "            images = self.transform(image=images)['image']\n",
    "              \n",
    "        #images = images.astype(np.float32)\n",
    "        #images /= 255\n",
    "        images = images.transpose(2, 0, 1)\n",
    "        \n",
    "        label = label.astype(np.float32)\n",
    "        #label2 = label2.astype(np.float32)\n",
    "        return torch.tensor(images), torch.tensor(label),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd5517a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:690: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "import albumentations\n",
    "\n",
    "transforms_train = albumentations.Compose([\n",
    "    albumentations.ShiftScaleRotate(scale_limit=0.3, rotate_limit=180,p=0.5),\n",
    "    A.Cutout(num_holes=12, max_h_size=4, max_w_size=4, fill_value=0, p=0.5),\n",
    "    #albumentations.Rotate(p=0.5),\n",
    "    #albumentations.Transpose(p=0.5),\n",
    "    #albumentations.VerticalFlip(p=0.5),\n",
    "    albumentations.HorizontalFlip(p=0.5),   \n",
    "    albumentations.Resize(imsize, imsize, p=1.0), \n",
    "    albumentations.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                       std=(0.229, 0.224, 0.225), p=1),\n",
    "])\n",
    "\n",
    "transforms_val = albumentations.Compose([albumentations.Resize(imsize, imsize, p=1.0),\n",
    "                                         albumentations.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                       std=(0.229, 0.224, 0.225), p=1),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3726718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "image must be numpy array type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3421/960782441.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_show\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_show\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#BGR2RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3421/3499083632.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# aug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#images = images.astype(np.float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to pass data to augmentations as named arguments, for example: aug(image=image)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"force_apply must have bool or int type\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mneed_to_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforce_apply\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m_check_args\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minternal_data_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchecked_single\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} must be numpy array type\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minternal_data_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchecked_multi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: image must be numpy array type"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_show = cifarDataset(train_df, transform=transforms_train)\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,10\n",
    "for i in range(3):\n",
    "    f, axarr = plt.subplots(1,5)\n",
    "    for p in range(5):\n",
    "        idx = np.random.randint(0, len(dataset_show))\n",
    "        img, label = dataset_show[idx]\n",
    "        img = img.flip(0) #BGR2RGB\n",
    "        axarr[p].imshow(img.transpose(0,1).transpose(1,2))\n",
    "        axarr[p].set_title(str(label))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be374443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "        Run one train epoch\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    lambda_alpha = 0.002\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        target2 = target\n",
    "\n",
    "        target = target.long().cuda()\n",
    "        target2 = target2.long().cuda()\n",
    "        input_var = input.cuda()\n",
    "        target_var = target\n",
    "\n",
    "        if not mixup:\n",
    "          alpha = 0\n",
    "        else:\n",
    "          alpha = 1\n",
    "        \n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # L2 regularization\n",
    "        l2_alpha = 0.0\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"alpha\" in name:\n",
    "                l2_alpha += torch.pow(param, 2)\n",
    "        loss += lambda_alpha * l2_alpha\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1))\n",
    "    return losses.avg\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    \"\"\"\n",
    "    Run evaluation\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.long().cuda()\n",
    "            input_var = input.cuda()\n",
    "            target_var = target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output.data, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                          i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                          top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'\n",
    "          .format(top1=top1))\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"alpha\" in name:\n",
    "            print(name, param.item())\n",
    "    return top1.avg, losses.avg\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth'):\n",
    "    \"\"\"\n",
    "    Save the training model\n",
    "    \"\"\"\n",
    "    torch.save(state, filename)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f005fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = cifarDataset(train_df[train_df.fold!=0], transform=transforms_train)\n",
    "val_dataset = cifarDataset(train_df[train_df.fold==0], transform=transforms_val, test=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=512, shuffle=True,\n",
    "        num_workers=8, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=512, shuffle=False,\n",
    "    num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4256dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "net = timm.create_model(\"vit_small_patch16_224\", pretrained=True)\n",
    "net.head = nn.Linear(net.head.in_features, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ffb1fd",
   "metadata": {},
   "source": [
    "# Train VIT without quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b809a63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marutema47\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/ken/dsk1/data/quantize_models_sandbox/wandb/run-20220617_154504-ul9jpaj3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/arutema47/pact_vit/runs/ul9jpaj3\" target=\"_blank\">vit_small_noquant</a></strong> to <a href=\"https://wandb.ai/arutema47/pact_vit\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-03\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3421/2660171307.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'current lr {:.5e}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "model = net.cuda()\n",
    "mixup = False\n",
    "\n",
    "import wandb\n",
    "watermark = \"vit_small_noquant\"\n",
    "wandb.init(project=\"pact_vit\",\n",
    "            name=watermark)\n",
    "\n",
    "# define loss function (criterion) and pptimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=0.0002)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                                    milestones=[80, 150],)\n",
    "\n",
    "best_prec1 = 0\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    # train for one epoch\n",
    "    print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
    "    tloss = train(train_loader, model, criterion, optimizer, epoch)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec1, valloss = validate(val_loader, model, criterion)\n",
    "\n",
    "    # wandb\n",
    "    wandb.log({'epoch': epoch, \"prec\":prec1, \"train_loss\": tloss, 'val_loss': valloss, \"lr\": optimizer.param_groups[0][\"lr\"],})\n",
    "\n",
    "    # remember best prec@1 and save checkpoint\n",
    "    is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "\n",
    "    print(\"Best prec1 : \", best_prec1)\n",
    "    if is_best:\n",
    "        torch.save(model.state_dict(), os.path.join(f'models/{watermark}.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f38958",
   "metadata": {},
   "source": [
    "# Train with quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bd81e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from module import ActFn\n",
    "\n",
    "# For activation quantization\n",
    "class Activation(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super().__init__()\n",
    "        self.ActFn = ActFn.apply\n",
    "        self.k = k\n",
    "        self.alpha = nn.Parameter(torch.tensor(10.))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.ActFn(x, self.alpha, self.k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b70d83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.layers import PatchEmbed, Mlp, DropPath, trunc_normal_, lecun_normal_\n",
    "from timm.models.registry import register_model\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "from timm.models.helpers import build_model_with_cfg, named_apply, adapt_input_conv\n",
    "from functools import partial\n",
    "from pact_utils import QuantizedLinear\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0., k=8, pact=False):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = QuantizedLinear(in_features, hidden_features, True, k, k, pact)\n",
    "        self.act = act_layer\n",
    "        self.fc2 = QuantizedLinear(hidden_features, out_features, True, k, k, pact)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "        # PACT\n",
    "        self.k = k\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0., k=8, pact=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = QuantizedLinear(dim, dim * 3, qkv_bias, k, k, pact)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = QuantizedLinear(dim, dim, True, k, k, pact)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "        # PACT\n",
    "        #self.alpha1 = nn.Parameter(torch.tensor(2.))\n",
    "        #self.alpha2 = nn.Parameter(torch.tensor(2.))\n",
    "        #self.alpha3 = nn.Parameter(torch.tensor(2.))\n",
    "        #self.alpha4 = nn.Parameter(torch.tensor(2.))\n",
    "        #self.quant = ActFn.apply\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, k=8, pact=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop, k=k, pact=pact)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop, k=k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer\n",
    "    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n",
    "        - https://arxiv.org/abs/2010.11929\n",
    "    Includes distillation token & head support for `DeiT: Data-efficient Image Transformers`\n",
    "        - https://arxiv.org/abs/2012.12877\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=True, representation_size=None, distilled=False,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., embed_layer=PatchEmbed, norm_layer=None,\n",
    "                 act_layer=None, weight_init='', k=8, pact=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            distilled (bool): model includes a distillation token and head as in DeiT models\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "            weight_init: (str): weight init scheme\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_tokens = 2 if distilled else 1\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "\n",
    "        self.patch_embed = embed_layer(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer, k=k, pact=pact)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        if representation_size and not distilled:\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                ('fc', nn.Linear(embed_dim, representation_size)),\n",
    "                ('act', nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.pre_logits = nn.Identity()\n",
    "\n",
    "        # Classifier head(s)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.head_dist = None\n",
    "        if distilled:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.init_weights(weight_init)\n",
    "\n",
    "    def init_weights(self, mode=''):\n",
    "        assert mode in ('jax', 'jax_nlhb', 'nlhb', '')\n",
    "        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        if self.dist_token is not None:\n",
    "            trunc_normal_(self.dist_token, std=.02)\n",
    "        if mode.startswith('jax'):\n",
    "            # leave cls token as zeros to match jax impl\n",
    "            named_apply(partial(_init_vit_weights, head_bias=head_bias, jax_impl=True), self)\n",
    "        else:\n",
    "            trunc_normal_(self.cls_token, std=.02)\n",
    "            self.apply(_init_vit_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        # this fn left here for compat with downstream users\n",
    "        _init_vit_weights(m)\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=''):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token', 'dist_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        if self.dist_token is None:\n",
    "            return self.head\n",
    "        else:\n",
    "            return self.head, self.head_dist\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        if self.num_tokens == 2:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        if self.dist_token is None:\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        if self.dist_token is None:\n",
    "            return self.pre_logits(x[:, 0])\n",
    "        else:\n",
    "            return x[:, 0], x[:, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if self.head_dist is not None:\n",
    "            x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple\n",
    "            if self.training and not torch.jit.is_scripting():\n",
    "                # during inference, return the average of both classifier predictions\n",
    "                return x, x_dist\n",
    "            else:\n",
    "                return (x + x_dist) / 2\n",
    "        else:\n",
    "            x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _init_vit_weights(module: nn.Module, name: str = '', head_bias: float = 0., jax_impl: bool = False):\n",
    "    \"\"\" ViT weight initialization\n",
    "    * When called without n, head_bias, jax_impl args it will behave exactly the same\n",
    "      as my original init for compatibility with prev hparam / downstream use cases (ie DeiT).\n",
    "    * When called w/ valid n (module name) and jax_impl=True, will (hopefully) match JAX impl\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        if name.startswith('head'):\n",
    "            nn.init.zeros_(module.weight)\n",
    "            nn.init.constant_(module.bias, head_bias)\n",
    "        elif name.startswith('pre_logits'):\n",
    "            lecun_normal_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "        else:\n",
    "            if jax_impl:\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    if 'mlp' in name:\n",
    "                        nn.init.normal_(module.bias, std=1e-6)\n",
    "                    else:\n",
    "                        nn.init.zeros_(module.bias)\n",
    "            else:\n",
    "                trunc_normal_(module.weight, std=.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    elif jax_impl and isinstance(module, nn.Conv2d):\n",
    "        # NOTE conv was left to pytorch default in my original init\n",
    "        lecun_normal_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm2d)):\n",
    "        nn.init.zeros_(module.bias)\n",
    "        nn.init.ones_(module.weight)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _load_weights(model: VisionTransformer, checkpoint_path: str, prefix: str = ''):\n",
    "    \"\"\" Load weights from .npz checkpoints for official Google Brain Flax implementation\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    def _n2p(w, t=True):\n",
    "        if w.ndim == 4 and w.shape[0] == w.shape[1] == w.shape[2] == 1:\n",
    "            w = w.flatten()\n",
    "        if t:\n",
    "            if w.ndim == 4:\n",
    "                w = w.transpose([3, 2, 0, 1])\n",
    "            elif w.ndim == 3:\n",
    "                w = w.transpose([2, 0, 1])\n",
    "            elif w.ndim == 2:\n",
    "                w = w.transpose([1, 0])\n",
    "        return torch.from_numpy(w)\n",
    "\n",
    "    w = np.load(checkpoint_path)\n",
    "    if not prefix and 'opt/target/embedding/kernel' in w:\n",
    "        prefix = 'opt/target/'\n",
    "\n",
    "    if hasattr(model.patch_embed, 'backbone'):\n",
    "        # hybrid\n",
    "        backbone = model.patch_embed.backbone\n",
    "        stem_only = not hasattr(backbone, 'stem')\n",
    "        stem = backbone if stem_only else backbone.stem\n",
    "        stem.conv.weight.copy_(adapt_input_conv(stem.conv.weight.shape[1], _n2p(w[f'{prefix}conv_root/kernel'])))\n",
    "        stem.norm.weight.copy_(_n2p(w[f'{prefix}gn_root/scale']))\n",
    "        stem.norm.bias.copy_(_n2p(w[f'{prefix}gn_root/bias']))\n",
    "        if not stem_only:\n",
    "            for i, stage in enumerate(backbone.stages):\n",
    "                for j, block in enumerate(stage.blocks):\n",
    "                    bp = f'{prefix}block{i + 1}/unit{j + 1}/'\n",
    "                    for r in range(3):\n",
    "                        getattr(block, f'conv{r + 1}').weight.copy_(_n2p(w[f'{bp}conv{r + 1}/kernel']))\n",
    "                        getattr(block, f'norm{r + 1}').weight.copy_(_n2p(w[f'{bp}gn{r + 1}/scale']))\n",
    "                        getattr(block, f'norm{r + 1}').bias.copy_(_n2p(w[f'{bp}gn{r + 1}/bias']))\n",
    "                    if block.downsample is not None:\n",
    "                        block.downsample.conv.weight.copy_(_n2p(w[f'{bp}conv_proj/kernel']))\n",
    "                        block.downsample.norm.weight.copy_(_n2p(w[f'{bp}gn_proj/scale']))\n",
    "                        block.downsample.norm.bias.copy_(_n2p(w[f'{bp}gn_proj/bias']))\n",
    "        embed_conv_w = _n2p(w[f'{prefix}embedding/kernel'])\n",
    "    else:\n",
    "        embed_conv_w = adapt_input_conv(\n",
    "            model.patch_embed.proj.weight.shape[1], _n2p(w[f'{prefix}embedding/kernel']))\n",
    "    model.patch_embed.proj.weight.copy_(embed_conv_w)\n",
    "    model.patch_embed.proj.bias.copy_(_n2p(w[f'{prefix}embedding/bias']))\n",
    "    model.cls_token.copy_(_n2p(w[f'{prefix}cls'], t=False))\n",
    "    pos_embed_w = _n2p(w[f'{prefix}Transformer/posembed_input/pos_embedding'], t=False)\n",
    "    if pos_embed_w.shape != model.pos_embed.shape:\n",
    "        pos_embed_w = resize_pos_embed(  # resize pos embedding when different size from pretrained weights\n",
    "            pos_embed_w, model.pos_embed, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)\n",
    "    model.pos_embed.copy_(pos_embed_w)\n",
    "    model.norm.weight.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/scale']))\n",
    "    model.norm.bias.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/bias']))\n",
    "    if isinstance(model.head, nn.Linear) and model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:\n",
    "        model.head.weight.copy_(_n2p(w[f'{prefix}head/kernel']))\n",
    "        model.head.bias.copy_(_n2p(w[f'{prefix}head/bias']))\n",
    "    if isinstance(getattr(model.pre_logits, 'fc', None), nn.Linear) and f'{prefix}pre_logits/bias' in w:\n",
    "        model.pre_logits.fc.weight.copy_(_n2p(w[f'{prefix}pre_logits/kernel']))\n",
    "        model.pre_logits.fc.bias.copy_(_n2p(w[f'{prefix}pre_logits/bias']))\n",
    "    for i, block in enumerate(model.blocks.children()):\n",
    "        block_prefix = f'{prefix}Transformer/encoderblock_{i}/'\n",
    "        mha_prefix = block_prefix + 'MultiHeadDotProductAttention_1/'\n",
    "        block.norm1.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale']))\n",
    "        block.norm1.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias']))\n",
    "        block.attn.qkv.weight.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('query', 'key', 'value')]))\n",
    "        block.attn.qkv.bias.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('query', 'key', 'value')]))\n",
    "        block.attn.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel']).flatten(1))\n",
    "        block.attn.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias']))\n",
    "        for r in range(2):\n",
    "            getattr(block.mlp, f'fc{r + 1}').weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/kernel']))\n",
    "            getattr(block.mlp, f'fc{r + 1}').bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/bias']))\n",
    "        block.norm2.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/scale']))\n",
    "        block.norm2.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/bias']))\n",
    "\n",
    "\n",
    "def resize_pos_embed(posemb, posemb_new, num_tokens=1, gs_new=()):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    _logger.info('Resized position embedding: %s to %s', posemb.shape, posemb_new.shape)\n",
    "    ntok_new = posemb_new.shape[1]\n",
    "    if num_tokens:\n",
    "        posemb_tok, posemb_grid = posemb[:, :num_tokens], posemb[0, num_tokens:]\n",
    "        ntok_new -= num_tokens\n",
    "    else:\n",
    "        posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
    "    gs_old = int(math.sqrt(len(posemb_grid)))\n",
    "    if not len(gs_new):  # backwards compatibility\n",
    "        gs_new = [int(math.sqrt(ntok_new))] * 2\n",
    "    assert len(gs_new) >= 2\n",
    "    _logger.info('Position embedding grid-size from %s to %s', [gs_old, gs_old], gs_new)\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=gs_new, mode='bicubic', align_corners=False)\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_new[0] * gs_new[1], -1)\n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    return posemb\n",
    "\n",
    "\n",
    "def checkpoint_filter_fn(state_dict, model):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    if 'model' in state_dict:\n",
    "        # For deit models\n",
    "        state_dict = state_dict['model']\n",
    "    for k, v in state_dict.items():\n",
    "        if 'patch_embed.proj.weight' in k and len(v.shape) < 4:\n",
    "            # For old models that I trained prior to conv based patchification\n",
    "            O, I, H, W = model.patch_embed.proj.weight.shape\n",
    "            v = v.reshape(O, -1, H, W)\n",
    "        elif k == 'pos_embed' and v.shape != model.pos_embed.shape:\n",
    "            # To resize pos embedding when using model at different size from pretrained weights\n",
    "            v = resize_pos_embed(\n",
    "                v, model.pos_embed, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def _create_vision_transformer(variant, pretrained=False, default_cfg=None, **kwargs):\n",
    "    default_cfg = default_cfg or default_cfgs[variant]\n",
    "    if kwargs.get('features_only', None):\n",
    "        raise RuntimeError('features_only not implemented for Vision Transformer models.')\n",
    "\n",
    "    # NOTE this extra code to support handling of repr size for in21k pretrained models\n",
    "    default_num_classes = default_cfg['num_classes']\n",
    "    num_classes = kwargs.get('num_classes', default_num_classes)\n",
    "    repr_size = kwargs.pop('representation_size', None)\n",
    "    if repr_size is not None and num_classes != default_num_classes:\n",
    "        # Remove representation layer if fine-tuning. This may not always be the desired action,\n",
    "        # but I feel better than doing nothing by default for fine-tuning. Perhaps a better interface?\n",
    "        _logger.warning(\"Removing representation layer for fine-tuning.\")\n",
    "        repr_size = None\n",
    "\n",
    "    model = build_model_with_cfg(\n",
    "        VisionTransformer, variant, pretrained,\n",
    "        default_cfg=default_cfg,\n",
    "        representation_size=repr_size,\n",
    "        pretrained_filter_fn=checkpoint_filter_fn,\n",
    "        pretrained_custom_load='npz' in default_cfg['url'],\n",
    "        **kwargs)\n",
    "    return model\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n",
    "        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "default_cfgs = {\n",
    "    # patch models (weights from official Google JAX impl)\n",
    "    'vit_tiny_patch16_224': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/'\n",
    "            'Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz'),\n",
    "    'vit_small_patch16_224': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/'\n",
    "            'S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz'),\n",
    "    }\n",
    "\n",
    "@register_model\n",
    "def vit_tiny_patch16_224(pretrained=False, k=8, pact=False, **kwargs):\n",
    "    \"\"\" ViT-Tiny (Vit-Ti/16)\n",
    "    \"\"\"\n",
    "    model_kwargs = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3, k=k, pact=pact, **kwargs)\n",
    "    model = _create_vision_transformer('vit_tiny_patch16_224', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def vit_small_patch16_224(pretrained=False, k=8, **kwargs):\n",
    "    \"\"\" ViT-Small (ViT-S/16)\n",
    "    NOTE I've replaced my previous 'small' model definition and weights with the small variant from the DeiT paper\n",
    "    \"\"\"\n",
    "    model_kwargs = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6, k=k, **kwargs)\n",
    "    model = _create_vision_transformer('vit_small_patch16_224', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf661401",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/vit_small_noquant.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3421/3252892594.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvit_small_patch16_224\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/vit_small_noquant.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/vit_small_noquant.pth'"
     ]
    }
   ],
   "source": [
    "for k in range(4,7):\n",
    "    net = vit_small_patch16_224(True, k=k, pact=False)\n",
    "    net.head = nn.Linear(net.head.in_features, 10)\n",
    "    checkpoint = torch.load(\"models/vit_small_noquant.pth\")\n",
    "    net.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "    model = net.cuda()\n",
    "    mixup = False\n",
    "\n",
    "    # Track experiment with wandb\n",
    "    import wandb\n",
    "    watermark = \"vit_small_k{}_nopact\".format(k)\n",
    "    wandb.init(project=\"pact_vit2\", name=watermark)\n",
    "\n",
    "    # define loss function (criterion) and pptimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    # optimizer for pact\n",
    "    # \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=0.0002)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                                        milestones=[5],)\n",
    "\n",
    "    best_prec1 = 0\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        # train for one epoch\n",
    "        print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
    "        tloss = train(train_loader, model, criterion, optimizer, epoch)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1, valloss = validate(val_loader, model, criterion)\n",
    "\n",
    "        # wandb\n",
    "        wandb.log({'epoch': epoch, \"prec\":prec1, \"train_loss\": tloss, 'val_loss': valloss, \"lr\": optimizer.param_groups[0][\"lr\"],})\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "\n",
    "        print(\"Best prec1 : \", best_prec1)\n",
    "        if is_best:\n",
    "            torch.save(model.state_dict(), os.path.join(f'models/{watermark}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8998c803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pg_utils import TorchQuantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa841328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TorchQuantize(bits=4).forward(torch.rand(1,384)).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "856f9b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mlp(\n",
       "  (fc1): QuantizedLinear(\n",
       "    in_features=384, out_features=1536, bias=True\n",
       "    (quantize_w): TorchQuantize(\n",
       "      (quantize): TorchRoundToBits()\n",
       "    )\n",
       "    (quantize_a): TorchQuantize(\n",
       "      (quantize): TorchRoundToBits()\n",
       "    )\n",
       "    (quantize_o): TorchQuantize(\n",
       "      (quantize): TorchRoundToBits()\n",
       "    )\n",
       "  )\n",
       "  (act): GELU()\n",
       "  (fc2): QuantizedLinear(\n",
       "    in_features=1536, out_features=384, bias=True\n",
       "    (quantize_w): TorchQuantize(\n",
       "      (quantize): TorchRoundToBits()\n",
       "    )\n",
       "    (quantize_a): TorchQuantize(\n",
       "      (quantize): TorchRoundToBits()\n",
       "    )\n",
       "    (quantize_o): TorchQuantize(\n",
       "      (quantize): TorchRoundToBits()\n",
       "    )\n",
       "  )\n",
       "  (drop): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[1].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d894c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.4530, -4.0587, -3.7705,  ...,  0.7073,  0.7283,  1.1625],\n",
       "       device='cuda:0', grad_fn=<Unique2Backward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[1].mlp.fc1.forward(torch.rand(1,384).cuda()).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75eb1382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedLinear(\n",
       "  in_features=384, out_features=1152, bias=True\n",
       "  (quantize_w): TorchQuantize(\n",
       "    (quantize): TorchRoundToBits()\n",
       "  )\n",
       "  (quantize_a): TorchQuantize(\n",
       "    (quantize): TorchRoundToBits()\n",
       "  )\n",
       "  (quantize_o): TorchQuantize(\n",
       "    (quantize): TorchRoundToBits()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[1].attn.qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba9662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
